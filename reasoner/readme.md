# Details of reasoner

### Data preparation
  1. The data sampling over gold proof tree will result in 8,819 instances. Each instance is a correct proof step from the golden tree in training split.
     
  2. Another data version would use the prompting proof steps from ChatGPT. This version is not adopted in final constrastive training.

### Reasoner training and inference
  1. Use the sampled data to train a `flan-t5-large` model as a verifier to generate hard negatives. The generated steps are around 8w negatives.  

### Vera filtering
  1. Check and filter the generated steps according to the score generated by `Vera (a 7B commonsense LLM)`. The filterd hard negatives are used in further finetuning.
